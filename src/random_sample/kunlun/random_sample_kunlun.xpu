#include "kunlun/common_kernel_kunlun.h"
#include "kunlun/reduce_kunlun.h"

template <typename Tval>
__device__ void swap_local(__local__ Tval &a, __local__ Tval &b) {
    __local__ Tval tmp = a;
    a = b;
    b = tmp;
}


template <typename Tval, typename Tidx>
__device__ void findTopk(
    __global_ptr__ Tval *values,
    __global_ptr__ Tidx *indices,
    int size,
    int topk) {
    __local__ Tval values_a;
    __local__ Tval values_b;
    __local__ Tidx indices_a;
    __local__ Tidx indices_b;
    for (int i = 0; i < topk; ++i) {
        for (int j = i + 1; j < size; ++j) {
            GM2LM(values + i, &values_a, sizeof(Tval));
            GM2LM(values + j, &values_b, sizeof(Tval));
            GM2LM(indices + i, &indices_a, sizeof(Tidx));
            GM2LM(indices + j, &indices_b, sizeof(Tidx));
            if constexpr(std::is_same_v<Tval, float>){
                if (values_a < values_b) {
                    swap_local(values_a, values_b);
                    swap_local(indices_a, indices_b);
                }
            }
            else if constexpr(std::is_same_v<Tval, half>){
                if (__half2float(values_a) < __half2float(values_b)) {
                    swap_local(values_a, values_b);
                    swap_local(indices_a, indices_b);
                }
            }
            /***
            else if constexpr(std::is_same_v<Tval, bfloat16_t>){
                if (__bfloat162float(values_a) < __bfloat162float(values_b)) {
                    swap_local(values_a, values_b);
                    swap_local(indices_a, indices_b);
                }
            }
            ***/
            LM2GM(&values_a, values + i, sizeof(Tval)); 
            LM2GM(&values_b, values + j, sizeof(Tval)); 
            LM2GM(&indices_a, indices + i, sizeof(Tidx));
            LM2GM(&indices_b, indices + j, sizeof(Tidx));
        }
    }
}

template <typename Tval, typename Tidx>
__device__ void findTopk_local(
    __local__ Tval *values,
    __local__ Tidx *result,
    int size,
    int topk) {
    for (int i = 0; i < topk; ++i) {
        for (int j = i + 1; j < size; ++j) {
            if constexpr(std::is_same_v<Tval, float>){
                if (values[i] < values[j]) {
                    swap_local(values[i], values[j]);
                    swap_local(result[i], result[j]);
                }
            }
            else if constexpr(std::is_same_v<Tval, half>){
                if (__half2float(values[i]) < __half2float(values[j])) {
                    swap_local(values[i], values[j]);
                    swap_local(result[i], result[j]);
                }
            }
            /***
            else if constexpr(std::is_same_v<Tval, bfloat16_t>){
                if (__bfloat162float(values[i]) < __bfloat162float(values[j])) {
                    swap_local(values[i], values[j]);
                    swap_local(result[i], result[j]);
                }
            }
            ***/
        }
    }
}

template <unsigned int BLOCK_SIZE, typename Tval, typename Tcompute, typename Tidx>
__global__ void random_sampleKernel(Tidx *result,
                                    const Tval *probs,
                                    float random_val,
                                    float topp,
                                    int voc,
                                    int topk,
                                    float temperature,
                                    Tidx *indices,
                                    Tval *values,
                                    Tidx *indices_global,
                                    Tval *values_global,
                                    Tcompute *sum_global) {
    int cid = core_id();
    if (cid >= BLOCK_SIZE) {
        return;
    }
    int thread_id = BLOCK_SIZE * cluster_id() + cid;
    int nthreads = BLOCK_SIZE * cluster_num();
    
    // 每个coreId分配step个元素
    int remain = voc % nthreads;
    int step_easy = (voc - remain) / nthreads;
    int step_hard = step_easy + 1;
    int step = (thread_id < remain ? step_hard : step_easy);
    int ind_start = (thread_id < remain ? thread_id * step_hard : remain * step_hard + (thread_id - remain) * step_easy);
    for (int index = ind_start; index < ind_start + step; index++) {
        indices[index] = index;
    }

    constexpr int buf_size = 128;
    __local__ Tval values_local[2 * buf_size];
    __local__ Tidx indices_local[2 * buf_size];
    for (int i = 0; i < 2 * buf_size; i++) {
        values_local[i] = -INFINITY;
        indices_local[i] = 0;
    }

    int remainTask = step % buf_size;
    int repeat = (step - remainTask) / buf_size;
    if (topk >= step_easy) {
        if (thread_id == 0) { 
            findTopk(values, indices, voc, topk);  
        }
        sync_cluster();
        for(int index = thread_id; index < topk; index += nthreads){
            GM2LM(values + index, values_local, sizeof(Tval));
            GM2LM(indices + index, indices_local, sizeof(Tidx));
            LM2GM(values_local, values_global + index, sizeof(Tval)); 
            LM2GM(indices_local, indices_global + index, sizeof(Tidx));
        }
        sync_cluster(); 
        
    } else {                        // topk < step_easy
        if (buf_size > step_easy) { // buf_size >= step_hard > step_easy > topk
            GM2LM(values + ind_start, values_local, step * sizeof(Tval));
            GM2LM(indices + ind_start, indices_local, step * sizeof(Tidx));
            findTopk_local(values_local, indices_local, step, topk);
            LM2GM(values_local, values_global + thread_id * topk, topk * sizeof(Tval)); // values_global前面nthreads * topk存储不同core的topk元素
            LM2GM(indices_local, indices_global + thread_id * topk, topk * sizeof(Tidx));
        } else {                   // buf_size <= step_easy
            if (topk > buf_size) { // step_easy > topk > buf_size
                
                findTopk(&values[ind_start], &indices[ind_start], step, topk);
                
                for(int r = 0; r < topk / buf_size + (topk % buf_size > 0 ? 1 : 0); r++){
                    int read_len = (r < topk / buf_size ? buf_size : topk % buf_size);
                    GM2LM(values + ind_start + r * buf_size, values_local, read_len * sizeof(Tval));
                    GM2LM(indices + ind_start + r * buf_size, indices_local, read_len * sizeof(Tidx));
                    LM2GM(values_local, values_global + thread_id * topk + r * buf_size, read_len * sizeof(Tval)); 
                    LM2GM(indices_local, indices_global + thread_id * topk + r * buf_size, read_len * sizeof(Tidx));
                }
            } else { // step_easy >= buf_size >= topk
                
                for (int r = 0; r < repeat; r++) {
                    GM2LM(values + ind_start + r * buf_size, values_local, buf_size * sizeof(Tval));
                    GM2LM(indices + ind_start + r * buf_size, indices_local, buf_size * sizeof(Tidx));
                    findTopk_local(values_local, indices_local, buf_size + topk, topk); // 每次循环把上次的前topk也加入对比
                    for (int i = buf_size; i < buf_size + topk; i++) {            // 把上一轮循环的topk加载到后半部分
                        values_local[i] = values_local[i - buf_size];
                        indices_local[i] = indices_local[i - buf_size];
                    }
                }
                if (remainTask) {
                    GM2LM(values + ind_start + repeat * buf_size, values_local, remainTask * sizeof(Tval));
                    GM2LM(indices + ind_start + repeat * buf_size, indices_local, remainTask * sizeof(Tidx));
                    findTopk_local(values_local, indices_local, buf_size + topk, topk);
                }
                LM2GM(values_local, values_global + thread_id * topk, topk * sizeof(Tval));
                LM2GM(indices_local, indices_global + thread_id * topk, topk * sizeof(Tidx));
            }
        }
        if (thread_id == 0) {
            findTopk(values_global, indices_global, nthreads * topk, topk);     
        }
    }
    
    //上面这部分是计算topk，数据分别存储在values_global,indices_global里面
    __global_ptr__ Tval *values_global_ = values_global;
    __shared__ Tval max_value;
    if(core_id() == 0){
        GM2SM(values_global, &max_value, sizeof(Tval));
    }
    sync_cluster();
    
    __shared__ Tval x_sm[SM_SIZE / sizeof(Tval)];
    __shared__ Tval y_sm[SM_SIZE / sizeof(Tval)];

    int sm_size = SM_SIZE / sizeof(Tval);
    int all_sm_size = cluster_num() * sm_size;
    int sm_remain = voc % all_sm_size;
    int sm_repeat = (voc - sm_remain) / all_sm_size;
    int sm_remain_cluster = sm_remain % cluster_num();
    int sm_step_easy = (sm_remain - sm_remain_cluster) / cluster_num();
    int sm_step_hard = sm_step_easy + 1;
    int sm_step = (cluster_id() < sm_remain_cluster ? sm_step_hard : sm_step_easy);
    int sm_ind_start = (cluster_id() < sm_remain_cluster ? cluster_id() * sm_step_hard : sm_remain_cluster * sm_step_hard + (cluster_id() - sm_remain_cluster) * sm_step_easy);
    
    
    __shared__ Tcompute sum_;
    if(cid == 0){
        sum_ = 0.0;
    }
    sync_cluster();
    __global_ptr__ Tval const *probs_ = probs;
    
    for (int r = 0; r < sm_repeat; r++) {
        if (cid == 0) {
            GM2SM_ASYNC(probs_ + r * all_sm_size + cluster_id() * sm_size, x_sm, sm_size * sizeof(Tval));
        }
        sync_cluster();
        
        for (int index = cid; index < sm_size; index += BLOCK_SIZE) {
            if constexpr (std::is_same_v<Tval, half>) {
                y_sm[index] = hexp((loadsm(x_sm + index) - loadsm(&max_value)) / to<half>(temperature));
            } else if constexpr (std::is_same_v<Tval, float>) {
                y_sm[index] = exp((x_sm[index] - max_value) / temperature);
            }
        }
        sync_cluster();
        
        Tcompute sum_0 = sum<BLOCK_SIZE, Tval, Tcompute>(y_sm, sm_size);
        __shared__ Tcompute sum_tmp_0;
        if (cid == 0) {
            sum_tmp_0 = sum_0;
            sum_ = loadsm(&sum_) + loadsm(&sum_tmp_0);
        }
        sync_cluster();
        
    }
    
    if (sm_step) {
        if (cid == 0) {
            GM2SM_ASYNC(probs_ + sm_repeat * all_sm_size + sm_ind_start, x_sm, sm_step * sizeof(Tval));
        }
        sync_cluster();
        for (int index = cid; index < sm_step; index += BLOCK_SIZE) {
            if constexpr (std::is_same_v<Tval, half>) {
                y_sm[index] = hexp((loadsm(x_sm + index) - loadsm(&max_value)) / to<half>(temperature));
            } else if constexpr (std::is_same_v<Tval, float>) {
                y_sm[index] = exp((x_sm[index] - max_value) / temperature);
            }
        }
        sync_cluster();

        Tcompute sum_0 = sum<BLOCK_SIZE, Tval, Tcompute>(y_sm, sm_step);
        __shared__ Tcompute sum_tmp_0;
        if (cid == 0) {
            sum_tmp_0 = sum_0;
            sum_ = loadsm(&sum_) + loadsm(&sum_tmp_0);
        }
        sync_cluster();
    }
    
    __global_ptr__ Tcompute *sum_global_ = sum_global;
    if (core_id() == 0) {
        SM2GM_ASYNC(&sum_, sum_global_  + cluster_id(), sizeof(Tcompute));
    }
    sync_cluster();
    
    __shared__ Tcompute all_sum;
    if(cid == 0){
        GM2SM_ASYNC(sum_global_, x_sm, cluster_num() * sizeof(Tcompute));
    }
    sync_cluster();
   
    Tcompute all_sum_0 = sum<BLOCK_SIZE, Tcompute, Tcompute>(x_sm, cluster_num());
    if (cid == 0) {
        all_sum = all_sum_0;
    }
    sync_cluster();
    
    if (thread_id == 0) {
        int end = topk;
        float cumsum = 0.0f;
        
        for(int r = 0; r < topk / buf_size + (topk % buf_size > 0 ? 1 : 0); r++){
            int read_len = (r < topk / buf_size ? buf_size : topk % buf_size);
            GM2LM(values_global + r * buf_size, values_local, read_len * sizeof(Tval));
            for (int index = 0; index < read_len; index++) {
                if constexpr (std::is_same_v<Tval, float>) {
                    cumsum += exp((values_local[index] - max_value) / temperature) / to<float>(loadsm(&all_sum));
                    
                } else if constexpr (std::is_same_v<Tval, half>) {
                    cumsum += exp((to<float>(values_local[index]) - to<float>(loadsm(&max_value))) / temperature) / to<float>(loadsm(&all_sum));
                }
                if (cumsum >= topp) {
                    end = r * buf_size + index + 1;
                    break;
                }
            }
        }
        random_val *= cumsum;
        cumsum = 0.0f;
        for(int r = 0; r < end / buf_size + (end % buf_size > 0 ? 1 : 0); r++){
            int read_len = (r < end / buf_size ? buf_size : end % buf_size);
            GM2LM(values_global + r * buf_size, values_local, read_len * sizeof(Tval));
            for (int index = 0; index < read_len; index++) {
                if constexpr (std::is_same_v<Tval, float>) {
                    cumsum += exp((values_local[index] - max_value) / temperature)/ to<float>(loadsm(&all_sum));
                } else if constexpr (std::is_same_v<Tval, half>) {
                    cumsum += exp((to<float>(values_local[index]) - to<float>(loadsm(&max_value))) / temperature)/ to<float>(loadsm(&all_sum));
                }
                if (random_val < cumsum) {
                    result[0] = indices_global[r * buf_size + index];
                    break;
                }
            }
        }
        
    }

}

template <typename Tval, typename Tidx>
void randomSampleKunlunDevice(void *result,
                  const void *probs,
                  float random_val,
                  float topp,
                  int voc,
                  int topk,
                  float temperature,
                  //xdnn::Context *handle,
                  XPUStream stream)
{
    
    int topk_ = topk <= voc ? topk : voc;
    bool dosample = topk_ > 1 && temperature != 0.0f && topp != 0.0f && random_val != 0.0f;
    int cluster_num = 256;
    int core_num = 64;
    size_t workspace_size = (voc + cluster_num * core_num * topk_ + cluster_num) * sizeof(Tval) + (voc + cluster_num * core_num * topk_) * sizeof(Tidx);
    char *workspace_value;
    xpu_malloc((void **)(&workspace_value), workspace_size);
    if (dosample)
    {
        constexpr unsigned int cluster_num = 8;
        constexpr unsigned int core_num = 64;
        Tval *values = (Tval *)workspace_value;// [voc]
        xpu_memcpy(values, probs, voc * sizeof(Tval), XPU_DEVICE_TO_DEVICE);
        Tval *values_global = values + voc;//[cluster_num * core_num * topk_]
        Tval *sum_global = values_global + cluster_num * core_num * topk_;//[cluster_num]
        char *workspace_index = workspace_value + (voc + cluster_num * core_num * topk_ + cluster_num) * sizeof(Tval);
        Tidx *indices = (Tidx *)workspace_index;//[voc]
        Tidx *indices_global = indices + voc;//[cluster_num * core_num * topk_]
        random_sampleKernel<core_num, Tval, Tval, Tidx><<<cluster_num, core_num, stream>>>((Tidx *)result,
                                                                                               (Tval *)probs,
                                                                                               random_val,
                                                                                               topp,
                                                                                               voc,
                                                                                               topk_,
                                                                                               temperature,
                                                                                               indices,
                                                                                               values,
                                                                                               indices_global,
                                                                                               values_global,
                                                                                                sum_global);
        xpu_wait(stream);                                                                                        
        
    }
    
}

template <typename Tval, typename Tidx>
void randomSampleKunlun(void *result,
                  const void *probs,
                  float random_val,
                  float topp,
                  int voc,
                  int topk,
                  float temperature){
                
    //auto handle = xdnn::create_context();
    XPUStream stream;
    int err = xpu_stream_create(&stream);
    if (sizeof(Tval) == 2){
        randomSampleKunlunDevice<half, Tidx>(result,
                                    probs,
                                    random_val,
                                    topp,
                                    voc,
                                    topk,
                                    temperature, stream);
    }
    else if (sizeof(Tval) == 4){
        randomSampleKunlunDevice<float, Tidx>(result,
                                    probs,
                                    random_val,
                                    topp,
                                    voc,
                                    topk,
                                    temperature, stream);
    }
    
    //destroy_context(handle);
}
extern "C" void randomSample_kunlun(void *result,
                                  const void *probs,
                                  float random_val,
                                  float topp,
                                  int voc,
                                  int topk,
                                  float temperature,
                                  int byteSize)
{

    if (byteSize == 2)
    {

        randomSampleKunlun<uint16_t, int>(result,
                               probs,
                               random_val,
                               topp,
                               voc,
                               topk,
                               temperature);
    }
    else
    {
        randomSampleKunlun<float, int>(result,
                            probs,
                            random_val,
                            topp,
                            voc,
                            topk,
                            temperature);
    }
}