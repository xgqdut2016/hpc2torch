#include "bang.h"
#include "cnrt.h"
#include "mlu/common_mlu.h"

const int NRAM_MAX_SIZE = 1024 * 256;
__nram__ char nram_buffer[NRAM_MAX_SIZE];

const int SRC_MAX_SIZE = 1024 * 32;//尽量取大一些


template <typename T, typename Tw>
__mlu_global__ void rmsnorm(T *destination, const T *source, Tw const *weight, int *shape, int *y_strides, int *x_strides, float eps, int ndim, int dim_s) {

    int other_size = 1;
    for (int i = 0; i < ndim - 1; i++) {
        other_size *= shape[i];
    }
    int dimsize = shape[ndim - 1];

    int max_num = (dimsize >= SRC_MAX_SIZE / sizeof(Tw) ? SRC_MAX_SIZE / sizeof(Tw) : dim_s); 
    constexpr int reduce_num = 128 / sizeof(float);

    int remain_task = other_size % taskDim;
    int step_easy = (other_size - remain_task) / taskDim;
    int step_hard = step_easy + 1;
    int step = (taskId < remain_task ? step_hard : step_easy);                                                                  // 每个taskId分别处理step个向量的reduce
    int ind_start = (taskId < remain_task ? taskId * step_hard : (taskId - remain_task) * step_easy + remain_task * step_hard); // 每个taskId处理数据的开头地址

    int offset = (sizeof(T) == 2 ? max_num : 0); // 这是为了后面使用float精度计算half数据做的处理
    char *nram_buffer_1 = nram_buffer + reduce_num * sizeof(float);
    char *nram_buffer_2 = nram_buffer_1 + (max_num + offset) * sizeof(T);
    float *dest_sum_final = (float *)nram_buffer;  //[reduce_num]，存储后面的reduce结果
    

    T *src = (T *)nram_buffer_1;   // 如果sizeof(T) = 2，此时src长度为2 max_num，如果sizeof(T) = 4，src长度为max_num
    Tw *wet = (Tw *)nram_buffer_2; // 长度是max_num

    int remain = dimsize % max_num;
    int repeat = (dimsize - remain) / max_num; // 一次搬运max_num，搬运dimsize个元素需要的次数

    for (int i = ind_start; i < ind_start + step; i++) {
        int ind_s = 0;
        int ind_d = 0;
        int ind_i = i;
        for (int j = ndim - 2; j >= 0; j--) {
            ind_s += (ind_i % shape[j]) * x_strides[j];
            ind_d += (ind_i % shape[j]) * y_strides[j];
            ind_i = ind_i / shape[j];
        }
        __bang_write_zero(dest_sum_final, reduce_num); // dest_sum_final[0]存储的是当前向量的规约值，因此每次循环都要初始化为0

        float global_sum = reduceSumSquare<T>(source + ind_s, src, dest_sum_final, dimsize, max_num);

        global_sum /= dimsize;
        global_sum += eps;
        global_sum = pow(global_sum, 0.5);
        float global_sum_inv = 1.0 / global_sum;
        if (remain) {
            __memcpy(src, source + ind_s + repeat * max_num, remain * sizeof(T), GDRAM2NRAM);
            __memcpy(wet, weight + repeat * max_num, remain * sizeof(Tw), GDRAM2NRAM);
            if constexpr (std::is_same<T, half>::value && std::is_same<Tw, float>::value) {
                __bang_float2half_dn((T *)wet, wet, max_num);
            }
            __bang_mul(src, src, (T *)wet, max_num); // src = src * wet
            __bang_mul_scalar(src, src, global_sum_inv, max_num);
            __memcpy(destination + ind_d + repeat * max_num, src, remain * sizeof(T), NRAM2GDRAM);
        }
        for (int s = 0; s < repeat; s++) {
            __memcpy(src, source + ind_s + s * max_num, max_num * sizeof(T), GDRAM2NRAM);
            __memcpy(wet, weight + s * max_num, max_num * sizeof(Tw), GDRAM2NRAM);
            if constexpr (std::is_same<T, half>::value && std::is_same<Tw, float>::value) {
                __bang_float2half_dn((T *)wet, wet, max_num);
            }
            __bang_mul(src, src, (T *)wet, max_num); // src = src * wet
            __bang_mul_scalar(src, src, global_sum_inv, max_num);
            __memcpy(destination + ind_d + s * max_num, src, max_num * sizeof(T), NRAM2GDRAM);
        }
        
    }
}

template<typename T, typename Tw>
void RMSNormUnion(cnrtQueue_t queue, void *y, void const *x, void const *w, int *shape, int *stride_y, int *stride_x, float eps, int ndim){
    cnrtDim3_t k_dim;
    cnrtFunctionType_t k_type;

    k_dim.x = 4;
    k_dim.y = 1;
    k_dim.z = 1;
    k_type = CNRT_FUNC_TYPE_UNION1;
    int dimsize = shape[ndim - 1];
    int dimS;
    float mi = log2(dimsize);
    if (floor(mi) == mi) {
        dimS = dimsize;
    } else {
        dimS = pow(2, floor(mi) + 1);
    }
    int wSize = 128 / sizeof(T);
    if (dimS < wSize){
        dimS = wSize;
    }
    
    auto y_ = reinterpret_cast<T *>(y);
    auto x_ = reinterpret_cast<T const *>(x);
    auto w_ = reinterpret_cast<Tw const *>(w);
    char *tmpDevice;
    CNRT_CHECK(cnrtMalloc((void**)&tmpDevice, 3 * ndim * sizeof(int)));
    char *tmpStride = tmpDevice + ndim * sizeof(int);
    int *mlu_shape = (int *)tmpDevice;
    int *mlu_stride_x = (int *)tmpStride;
    int *mlu_stride_y = mlu_stride_x + ndim;
    CNRT_CHECK(cnrtMemcpy(mlu_shape, shape, ndim * sizeof(int), cnrtMemcpyHostToDev));
    CNRT_CHECK(cnrtMemcpy(mlu_stride_x, stride_x, ndim * sizeof(int), cnrtMemcpyHostToDev));
    CNRT_CHECK(cnrtMemcpy(mlu_stride_y, stride_y, ndim * sizeof(int), cnrtMemcpyHostToDev));

    rmsnorm<T, Tw><<<k_dim, k_type, queue>>>(y_, x_, w_, mlu_shape, mlu_stride_y, mlu_stride_x, eps, ndim, dimS);
    
    cnrtQueueSync(queue);
    cnrtFree(tmpDevice);
}
extern "C" void RMSNorm_bang(void *y, void const *x, void const *w,
                             int *shape, int *stride_y, int *stride_x, float eps, int ndim, int byteT, int byteTw){
    cnrtQueue_t queue;
    CNRT_CHECK(cnrtSetDevice(0));
    CNRT_CHECK(cnrtQueueCreate(&queue));
    
    if(byteT == 2 && byteTw == 4){
        RMSNormUnion<half, float>(queue, y, x, w, shape, stride_y, stride_x, eps, ndim);
    }
    
    else if(byteT == 4 && byteTw == 4){
        RMSNormUnion<float, float>(queue, y, x, w, shape, stride_y, stride_x, eps, ndim);
    }
    else if(byteT == 2 && byteTw == 2){
        RMSNormUnion<half, half>(queue, y, x, w, shape, stride_y, stride_x, eps, ndim);
    }
    CNRT_CHECK(cnrtQueueDestroy(queue));

}


