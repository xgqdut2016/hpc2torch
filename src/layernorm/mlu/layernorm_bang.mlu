#include "bang.h"
#include "cnrt.h"
const int NRAM_MAX_SIZE = 1024 * 256;
__nram__ char nram_buffer[NRAM_MAX_SIZE];

template<typename T>
__mlu_global__ void layernormKernel(T const *input, T const *scale, T const *bias, T *output, T *tmpGdram, float eps, int size, int behindsize, int bSize){
    int frontsize = size / behindsize;
    const int SRC_MAX_SIZE = NRAM_MAX_SIZE / 16;
    const int wSize = 128 / sizeof(T);

    const int maxNum = SRC_MAX_SIZE / sizeof(T);
    

    T *src = (T *)nram_buffer;//[maxNum]
    T *destSum = src + 3 * maxNum;//[3 * maxNum]
    T *destSumFinal = destSum + maxNum;//[wSize]
    T *s_src = destSumFinal + wSize;//[3 * maxNum]
    T *b_src = s_src + 3 * maxNum;//[3 * maxNum]
    //bSize是大于等于behindsize的最小2次幂
   
    if (behindsize >= taskDim * maxNum){
        //__bang_printf("xiao, frontsize:%d\n", frontsize);
        int segNum = maxNum / wSize;
        int taskSize = taskDim * maxNum;
        int remain = behindsize % taskSize;
        int repeat = (behindsize - remain) / taskSize;

        int remainT = remain % taskDim;
        int stepEasy = (remain - remainT) / taskDim;
        int stepHard = stepEasy + 1;
        int step = (taskId < remainT ? stepHard : stepEasy);
        int indStart = repeat * taskSize + (taskId < remainT ? taskId * stepHard : (remainT * stepHard + (taskId - remainT) * stepEasy));
        for(int i = 0; i < frontsize; i++){
            int tid = i * behindsize;
            __bang_write_zero(destSum, maxNum);
            __bang_write_zero(destSumFinal, wSize);
            for(int j = 0; j < repeat + 1; j++){
                if(j < repeat){
                    __memcpy_async(src + j % 2 * maxNum, input + tid + j * taskSize + taskId * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                }
                if(j > 0){
                    __bang_add(destSum, destSum, src + (j - 1) % 2 * maxNum, maxNum);
                }
                __sync_all_ipu();
            }
            if(step){
                __memcpy(src, input + tid + indStart, step * sizeof(T), GDRAM2NRAM);
                __bang_add(destSum, destSum, src, step);
            }
            __bang_mul_scalar(destSum, destSum, 1.0 / behindsize, maxNum);
            for(int strip = segNum/2; strip > 0; strip = strip / 2){
                for(int i = 0; i < strip ; i++){
                    __bang_add(destSum + i * wSize, destSum + i * wSize, destSum + (i + strip) * wSize, wSize);
                } 
            }
            __bang_reduce_sum(destSumFinal, destSum, wSize);//destSumFinal[0]存储的是当前task对应数据的规约和
            tmpGdram[taskId] = destSumFinal[0];
            __sync_all();//这个函数只能同步同一个cluster的所有core，因此最好使用taskDim = 4
            
            __bang_write_zero(destSum, maxNum);
            __bang_write_zero(destSumFinal, wSize);
            __memcpy(destSum, tmpGdram, taskDim * sizeof(T), GDRAM2NRAM);
            __bang_reduce_sum(destSumFinal, destSum, wSize);
            T mu = destSumFinal[0];
            
           
            
            //下面计算方差
            __bang_write_zero(destSum, maxNum);
            __bang_write_zero(destSumFinal, wSize);
            for(int j = 0; j < repeat + 1; j++){
                if (j < repeat){
                    __memcpy_async(src + j % 2 * maxNum, input + tid + j * taskSize + taskId * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                }
                if(j > 0){
                    __bang_sub_scalar(src + (j - 1) % 2 * maxNum, src + (j - 1) % 2 * maxNum, mu, maxNum);
                    __bang_mul(src + (j - 1) % 2 * maxNum, src + (j - 1) % 2 * maxNum, src + (j - 1) % 2 * maxNum, maxNum);
                    __bang_add(destSum, destSum, src + (j - 1) % 2 * maxNum, maxNum);
                }
                __sync_all_ipu();
            }
            if (step){
                
                __memcpy(src, input + tid + indStart, step * sizeof(T), GDRAM2NRAM);
                __bang_sub_scalar(src, src, mu, step);
                __bang_mul(src, src, src, step);
                __bang_add(destSum, destSum, src, step);
            }
            __bang_mul_scalar(destSum, destSum, 1.0 / behindsize, maxNum);
            for(int strip = segNum/2; strip > 0; strip = strip / 2){
                for(int i = 0; i < strip ; i++){
                    __bang_add(destSum + i * wSize, destSum + i * wSize, destSum + (i + strip) * wSize, wSize);
                } 
            }
            __bang_reduce_sum(destSumFinal, destSum, wSize);//destSumFinal[0]存储的是当前task对应数据的规约和
            
            tmpGdram[taskId] = destSumFinal[0];
            __sync_all();
            __bang_write_zero(destSum, maxNum);
            __bang_write_zero(destSumFinal, wSize);
            __memcpy(destSum, tmpGdram, taskDim * sizeof(T), GDRAM2NRAM);
            __bang_reduce_sum(destSumFinal, destSum, wSize);
            T sigma2 = destSumFinal[0] + static_cast<T>(eps);
            
            sigma2 = 1.0 / pow(sigma2, 0.5);
            //下面开始做变换
            for(int j = 0; j < repeat + 2; j++){
                if(j < repeat){
                    __memcpy_async(src + j % 3 * maxNum, input + tid + j * taskSize + taskId * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                    __memcpy_async(s_src + j % 3 * maxNum, scale + j * taskSize + taskId * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                    __memcpy_async(b_src + j % 3 * maxNum, bias + j * taskSize + taskId * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                }
                if(j > 0 && j < repeat + 1){
                    __bang_sub_scalar(src + (j - 1) % 3 * maxNum, src + (j - 1) % 3 * maxNum, mu, maxNum);
                    __bang_mul_scalar(src + (j - 1) % 3 * maxNum, src + (j - 1) % 3 * maxNum, sigma2, maxNum);
                    __bang_mul(src + (j - 1) % 3 * maxNum, src + (j - 1) % 3 * maxNum, s_src + (j - 1) % 3 * maxNum, maxNum);
                    __bang_add(src + (j - 1) % 3 * maxNum, src + (j - 1) % 3 * maxNum, b_src + (j - 1) % 3 * maxNum, maxNum);
                }
                if(j > 1){
                    __memcpy_async(output + tid + (j - 2) * taskSize + taskId * maxNum, src + (j - 2) % 3 * maxNum, maxNum * sizeof(T), NRAM2GDRAM);
                }
                __sync_all_ipu();
            }
            if (step){
                __memcpy(src, input + tid + indStart, step * sizeof(T), GDRAM2NRAM);
                __memcpy(s_src, scale + indStart, step * sizeof(T), GDRAM2NRAM);
                __memcpy(b_src, bias + indStart, step * sizeof(T), GDRAM2NRAM);
                __bang_sub_scalar(src, src, mu, maxNum);
                __bang_mul_scalar(src, src, sigma2, maxNum);
                __bang_mul(src, src, s_src, maxNum);
                __bang_add(src, src, b_src, maxNum);
                __memcpy(output + tid + indStart, src, step * sizeof(T), NRAM2GDRAM);
            }
        }
    }
    else if(behindsize >= maxNum && behindsize < taskDim * maxNum){
        int segNum = maxNum / wSize;
        int remainT = behindsize % maxNum;
        int repeat = (behindsize - remainT) / maxNum;

        int remain = frontsize % taskDim;
        int stepEasy = (frontsize - remain) / taskDim;
        int stepHard = stepEasy + 1;
        int step = (taskId < remain ? stepHard : stepEasy);
        int indStart = (taskId < remain ? taskId * stepHard : (remain * stepHard + (taskId - remain) * stepEasy));
        for(int i = indStart; i < indStart + step; i++){
            int tid = i * behindsize;
            //下面开始计算均值
            __bang_write_zero(destSum, maxNum);
            __bang_write_zero(destSumFinal, wSize);
            for(int j = 0; j < repeat + 1; j++){
                if (j < repeat){
                    __memcpy_async(src + j % 2 * maxNum, input + tid + j * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                }
                if(j > 0){
                    __bang_add(destSum, destSum, src + (j - 1) % 2 * maxNum, maxNum);
                }
                __sync_all_ipu();
            }
            if (remainT){
                
                __memcpy(src, input + tid + repeat * maxNum, remainT * sizeof(T), GDRAM2NRAM);
                __bang_add(destSum, destSum, src, remainT);
            }
            
            for(int strip = segNum/2; strip > 0; strip = strip / 2){
                for(int i = 0; i < strip ; i++){
                    __bang_add(destSum + i * wSize, destSum + i * wSize, destSum + (i + strip) * wSize, wSize);
                } 
            }
            __bang_reduce_sum(destSumFinal, destSum, wSize);
            //下面开始计算方差,destSumFinal[0]存储的就是均值
            T mu = destSumFinal[0] / behindsize;
            __bang_write_zero(destSum, maxNum);
            __bang_write_zero(destSumFinal, wSize);
            for(int j = 0; j < repeat + 1; j++){
                if(j < repeat){
                    __memcpy_async(src + j % 2 * maxNum, input + tid + j * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                }
                if(j > 0){
                    __bang_sub_scalar(src + (j - 1) % 2 * maxNum, src + (j - 1) % 2 * maxNum, mu, maxNum);
                    __bang_mul(src + (j - 1) % 2 * maxNum, src + (j - 1) % 2 * maxNum, src + (j - 1) % 2 * maxNum, maxNum);
                    __bang_add(destSum, destSum, src + (j - 1) % 2 * maxNum, maxNum);
                }
                __sync_all_ipu();
            }
            if (remainT){
                
                __memcpy(src, input + tid + repeat * maxNum, remainT * sizeof(T), GDRAM2NRAM);
                __bang_sub_scalar(src, src, mu, remainT);
                __bang_mul(src, src, src, remainT);
                __bang_add(destSum, destSum, src, remainT);
            }
            
            for(int strip = segNum/2; strip > 0; strip = strip / 2){
                for(int i = 0; i < strip ; i++){
                    __bang_add(destSum + i * wSize, destSum + i * wSize, destSum + (i + strip) * wSize, wSize);
                } 
            }
            __bang_reduce_sum(destSumFinal, destSum, wSize);
            T sigma2 = destSumFinal[0] / behindsize + static_cast<T>(eps);
            sigma2 = 1.0 / pow(sigma2, 0.5);
            //下面开始做变换
            for(int j = 0; j < repeat + 2; j++){
                if(j < repeat){
                    __memcpy_async(src + j % 3 * maxNum, input + tid + j * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                    __memcpy_async(s_src + j % 3 * maxNum, scale + j * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                    __memcpy_async(b_src + j % 3 * maxNum, bias + j * maxNum, maxNum * sizeof(T), GDRAM2NRAM);
                }
                if(j > 0 && j < repeat + 1){
                    __bang_sub_scalar(src + (j - 1) % 3 * maxNum, src + (j - 1) % 3 * maxNum, mu, maxNum);
                    __bang_mul_scalar(src + (j - 1) % 3 * maxNum, src + (j - 1) % 3 * maxNum, sigma2, maxNum);
                    __bang_mul(src + (j - 1) % 3 * maxNum, src + (j - 1) % 3 * maxNum, s_src + (j - 1) % 3 * maxNum, maxNum);
                    __bang_add(src + (j - 1) % 3 * maxNum, src + (j - 1) % 3 * maxNum, b_src + (j - 1) % 3 * maxNum, maxNum);
                }
                if(j > 1){
                    __memcpy_async(output + tid + (j - 2) * maxNum, src + (j - 2) % 3 * maxNum, maxNum * sizeof(T), NRAM2GDRAM);
                }
                __sync_all_ipu();
            }
            if(remainT){
                __memcpy(src, input + tid + repeat * maxNum, remainT * sizeof(T), GDRAM2NRAM);
                __memcpy(s_src, scale + repeat * maxNum, remainT * sizeof(T), GDRAM2NRAM);
                __memcpy(b_src, bias + repeat * maxNum, remainT * sizeof(T), GDRAM2NRAM);
                __bang_sub_scalar(src, src, mu, maxNum);
                __bang_mul_scalar(src, src, sigma2, maxNum);
                __bang_mul(src, src, s_src, maxNum);
                __bang_add(src, src, b_src, maxNum);
                __memcpy(output + tid + repeat * maxNum, src, remainT * sizeof(T), NRAM2GDRAM);
            }
        }
    }
    else{
        int multiple = maxNum / behindsize;//一个core一次可以处理multiple个behindsize
        int taskSize = taskDim * multiple;
        int remainT = frontsize % taskSize;
        int repeat = (frontsize - remainT) / taskSize;
        int remain = remainT % taskDim;
        int stepEasy = (remainT - remain) / taskDim;
        int stepHard = stepEasy + 1;
        int step = (taskId < remain ? stepHard : stepEasy);
        int indStart = (taskId < remain ? taskId * stepHard : (remain * stepHard + (taskId - remain) * stepEasy));
        int segNum = bSize / wSize;
        __memcpy(s_src, scale, behindsize * sizeof(T), GDRAM2NRAM);
        __memcpy(b_src, bias, behindsize * sizeof(T), GDRAM2NRAM);
        int tid;
        for(int i = 0; i < repeat + 2; i++){
            if(i < repeat){
                tid = i * taskSize * behindsize;
                __memcpy_async(src + i % 3 * maxNum, input + tid + taskId * multiple * behindsize, multiple * behindsize * sizeof(T), GDRAM2NRAM);
            }
            if(i > 0 && i < repeat + 1){
                for(int m = 0; m < multiple; m++){
                    __bang_write_zero(destSum, maxNum);
                    __bang_write_zero(destSumFinal, wSize);
                    __bang_add(destSum, destSum, src + (i - 1) % 3 * maxNum + m *behindsize, behindsize);
                    for(int strip = segNum/2; strip > 0; strip = strip / 2){
                        for(int i = 0; i < strip ; i++){
                            __bang_add(destSum + i * wSize, destSum + i * wSize, destSum + (i + strip) * wSize, wSize);
                        } 
                    }
                    __bang_reduce_sum(destSumFinal, destSum, wSize);//destSumFinal[0] / behindsize = mu
                    T mu = destSumFinal[0] / behindsize;
                    __bang_write_zero(destSum, maxNum);
                    __bang_sub_scalar(destSum, src + (i - 1) % 3 * maxNum + m * behindsize, mu, behindsize);
                    
                    __bang_mul(destSum, destSum, destSum, bSize);
                    __bang_write_zero(destSumFinal, wSize);
                    for(int strip = segNum/2; strip > 0; strip = strip / 2){
                        for(int i = 0; i < strip ; i++){
                            __bang_add(destSum + i * wSize, destSum + i * wSize, destSum + (i + strip) * wSize, wSize);
                        } 
                    }
                    
                    __bang_reduce_sum(destSumFinal, destSum, wSize);
                    T sigma2 = 1.0 / (pow(destSumFinal[0] / behindsize + static_cast<T>(eps), 0.5));
                    //下面开始做变换
                    __bang_sub_scalar(src + (i - 1) % 3 * maxNum + m * behindsize, src + (i - 1) % 3 * maxNum + m * behindsize, mu, behindsize);
                    __bang_mul_scalar(src + (i - 1) % 3 * maxNum + m * behindsize, src + (i - 1) % 3 * maxNum + m * behindsize, sigma2, behindsize);
                    __bang_mul(src + (i - 1) % 3 * maxNum + m * behindsize, src + (i - 1) % 3 * maxNum + m * behindsize, s_src, behindsize);
                    __bang_add(src + (i - 1) % 3 * maxNum + m * behindsize, src + (i - 1) % 3 * maxNum + m * behindsize, b_src, behindsize);
                }
            }
            if(i > 1){
                tid = (i - 2) * taskSize * behindsize;
                __memcpy_async(output + tid + taskId * multiple * behindsize, src + (i - 2) % 3 * maxNum, multiple * behindsize * sizeof(T), NRAM2GDRAM);
            }
            __sync_all_ipu();
        }
        if(step){
            int tid = (repeat * taskSize + indStart) * behindsize;
            __memcpy(src, input + tid, step * behindsize * sizeof(T), GDRAM2NRAM);
            for(int m = 0; m < step; m++){
                __bang_write_zero(destSum, maxNum);
                __bang_write_zero(destSumFinal, wSize);
                __bang_add(destSum, destSum, src + m *behindsize, behindsize);
                for(int strip = segNum/2; strip > 0; strip = strip / 2){
                    for(int i = 0; i < strip ; i++){
                        __bang_add(destSum + i * wSize, destSum + i * wSize, destSum + (i + strip) * wSize, wSize);
                    } 
                }
                __bang_reduce_sum(destSumFinal, destSum, wSize);//destSumFinal[0] / behindsize = mu
                T mu = destSumFinal[0] / behindsize;
                __bang_write_zero(destSum, maxNum);
                __bang_sub_scalar(destSum, src + m * behindsize, mu, behindsize);
                
                __bang_mul(destSum, destSum, destSum, bSize);
                __bang_write_zero(destSumFinal, wSize);
                for(int strip = segNum/2; strip > 0; strip = strip / 2){
                    for(int i = 0; i < strip ; i++){
                        __bang_add(destSum + i * wSize, destSum + i * wSize, destSum + (i + strip) * wSize, wSize);
                    } 
                }
                
                __bang_reduce_sum(destSumFinal, destSum, wSize);
                T sigma2 = 1.0 / (pow(destSumFinal[0] / behindsize + static_cast<T>(eps), 0.5));
                //下面开始做变换
                __bang_sub_scalar(src + m * behindsize, src + m * behindsize, mu, behindsize);
                __bang_mul_scalar(src + m * behindsize, src + m * behindsize, sigma2, behindsize);
                __bang_mul(src + m * behindsize, src + m * behindsize, s_src, behindsize);
                __bang_add(src + m * behindsize, src + m * behindsize, b_src, behindsize);

            }
            __memcpy(output + tid, src, step * behindsize * sizeof(T), NRAM2GDRAM);
        }
    }
}
template<typename T>
void layernormUnion(cnrtQueue_t queue, void const *input, void const *scale, void const *bias, void *output, float eps, int size, int behindsize){
    int wSize = 128 / sizeof(T);
    int bSize;
    float mi = log2(behindsize);
    if (floor(mi) == mi)
    {
        bSize = behindsize;
    }
    else
    {
        bSize = static_cast<int>(pow(2, floor(mi) + 1));
    }
    if (bSize < wSize)
    {
        bSize = wSize;
    }
    auto source = reinterpret_cast<const T *>(input);
    auto weight = reinterpret_cast<const T *>(scale);
    auto _bias = reinterpret_cast<const T *>(bias);
    auto destination = reinterpret_cast<T *>(output);

    cnrtDim3_t k_dim;
    cnrtFunctionType_t k_type;

    k_dim.x = 4;//由于sync_all()只能同步同一个cluster内所有core，因此涉及core外同步的时候必须选择taskDim=4才能保证同步不出错
    k_dim.y = 1;
    k_dim.z = 1;
    int taskNum = k_dim.x * k_dim.y * k_dim.z;

    k_type = CNRT_FUNC_TYPE_UNION1;
    T *tmpGdram;
    CNRT_CHECK(cnrtMalloc((void **)&tmpGdram, taskNum * sizeof(T)));
    layernormKernel<T><<<k_dim, k_type, queue>>>(source, weight, _bias, destination, tmpGdram, eps, size, behindsize, bSize);
    cnrtFree(tmpGdram);
    cnrtQueueSync(queue);
}
extern "C" void layernorm_bang(void const *input, void const *scale, void const *bias, void *output, 
float eps, int size, int behindsize, int byteSize){
    cnrtQueue_t queue;
    CNRT_CHECK(cnrtSetDevice(0));
    CNRT_CHECK(cnrtQueueCreate(&queue));
    if (byteSize == 2)
    {
        layernormUnion<half>(queue, input, scale, bias, output, eps, size, behindsize);
    }
    else if (byteSize == 4)
    {
        layernormUnion<float>(queue, input, scale, bias, output, eps, size, behindsize);
    }
    
    CNRT_CHECK(cnrtQueueDestroy(queue));
}

